#!/usr/bin/env python3
"""
Organize already-uploaded Google Photos files into albums based on FTP manifest.

This script:
1. Loads the FTP structure manifest (JSON) generated by generate_ftp_tree.py
2. Maps ISO files to MKV equivalents (since .iso files were converted during upload)
3. Creates albums matching the FTP folder structure
4. Moves uploaded files into their corresponding albums

The script handles the fact that all files were initially uploaded to the main
Google Photos library without folder structure, and now need to be organized.
"""

import os
import sys
import json
import logging
import hashlib
import sqlite3
from pathlib import Path
from typing import List, Dict, Optional, Set
from collections import defaultdict

try:
    from gpmc import Client
except ImportError:
    print("ERROR: gpmc library not installed. Install with: pip install https://github.com/xob0t/google_photos_mobile_client/archive/refs/heads/main.zip")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('organize_gphotos.log')
    ]
)
logger = logging.getLogger(__name__)


class PhotoOrganizer:
    """Organizes Google Photos files based on FTP manifest structure."""
    
    def __init__(self, auth_data: str, manifest_path: str):
        """
        Initialize the organizer.
        
        Args:
            auth_data: Google Photos authentication data
            manifest_path: Path to FTP structure manifest JSON file
        """
        self.auth_data = auth_data
        self.manifest_path = manifest_path
        self.client = None
        self.manifest = None
        self.file_to_album_map = {}
        self.media_cache = {}  # Cache of filename -> media_key
        self.gpmc_cache_path = None  # Path to gpmc cache database
        self.gpmc_schema_checked = False  # Track if we've validated the schema
        
    def load_manifest(self) -> bool:
        """Load the FTP structure manifest from JSON file."""
        try:
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                self.manifest = json.load(f)
            
            logger.info(f"‚úÖ Loaded manifest from {self.manifest_path}")
            logger.info(f"   Server: {self.manifest['metadata']['server']}")
            logger.info(f"   Total files: {self.manifest['statistics']['total_files']}")
            logger.info(f"   Total size: {self.manifest['statistics']['total_size_gb']} GB")
            return True
        except FileNotFoundError:
            logger.error(f"‚ùå Manifest file not found: {self.manifest_path}")
            return False
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Invalid JSON in manifest file: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error loading manifest: {e}")
            return False
    
    def initialize_client(self) -> bool:
        """Initialize Google Photos client."""
        try:
            self.client = Client(auth_data=self.auth_data)
            logger.info("‚úÖ Google Photos client initialized")
            
            # Set the gpmc cache path (same location gpmc uses)
            self.gpmc_cache_path = Path.home() / ".cache" / "gpmc" / "library.db"
            if self.gpmc_cache_path.exists():
                logger.info(f"‚úÖ Found gpmc cache database at {self.gpmc_cache_path}")
            else:
                logger.warning(f"‚ö†Ô∏è gpmc cache database not found at {self.gpmc_cache_path}")
                logger.warning("   Consider running 'gpmc update-cache' to build the cache")
                
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Google Photos client: {e}")
            return False
    
    def map_iso_to_mkv(self, filename: str) -> str:
        """
        Map an ISO filename to its MKV equivalent.
        
        During upload, .iso files were converted to .mkv with same base name.
        
        Args:
            filename: Original filename (e.g., "Movie.iso")
            
        Returns:
            MKV filename (e.g., "Movie.mkv")
        """
        if filename.lower().endswith('.iso'):
            return filename[:-4] + '.mkv'
        return filename
    
    def check_gpmc_cache_schema(self) -> bool:
        """
        Check if the gpmc cache database has the expected schema.
        Logs helpful information if the schema is different.
        
        Returns:
            True if schema looks correct, False otherwise
        """
        if self.gpmc_schema_checked:
            return True
            
        if not self.gpmc_cache_path or not self.gpmc_cache_path.exists():
            return False
        
        try:
            conn = sqlite3.connect(self.gpmc_cache_path)
            cursor = conn.cursor()
            
            # Check if remote_media table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='remote_media'"
            )
            if not cursor.fetchone():
                logger.warning("‚ö†Ô∏è gpmc cache database doesn't have 'remote_media' table")
                logger.warning("   Run 'gpmc update-cache' to build the cache properly")
                conn.close()
                return False
            
            # Check the schema of remote_media table
            cursor.execute("PRAGMA table_info(remote_media)")
            columns = cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            logger.debug(f"  gpmc cache columns: {column_names}")
            
            # Check for required columns
            if 'file_name' not in column_names:
                logger.warning("‚ö†Ô∏è gpmc cache 'remote_media' table missing 'file_name' column")
                logger.warning(f"   Available columns: {column_names}")
                logger.warning("   Run 'gpmc update-cache' to rebuild the cache")
                conn.close()
                return False
            
            if 'media_key' not in column_names:
                logger.warning("‚ö†Ô∏è gpmc cache 'remote_media' table missing 'media_key' column")
                logger.warning(f"   Available columns: {column_names}")
                logger.warning("   Run 'gpmc update-cache' to rebuild the cache")
                conn.close()
                return False
            
            # Check if there's any data
            cursor.execute("SELECT COUNT(*) FROM remote_media")
            count = cursor.fetchone()[0]
            logger.info(f"‚úÖ gpmc cache has {count} media items")
            
            if count == 0:
                logger.warning("‚ö†Ô∏è gpmc cache is empty - run 'gpmc update-cache' to populate it")
            
            conn.close()
            self.gpmc_schema_checked = True
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to check gpmc cache schema: {e}")
            return False
    
    def search_media_key_in_gpmc_cache(self, filename: str) -> Optional[str]:
        """
        Search for a media key by filename in the gpmc local cache database.
        
        This serves as a fallback when the upload_state.json doesn't have the media key.
        Uses multiple search strategies to find the file:
        1. Exact filename match (case-sensitive)
        2. Case-insensitive filename match
        3. Basename match (filename only, no path)
        4. LIKE pattern match for files ending with the filename
        
        Args:
            filename: The filename to search for
            
        Returns:
            The media_key if found, None otherwise
        """
        if not self.gpmc_cache_path or not self.gpmc_cache_path.exists():
            logger.debug(f"  gpmc cache not available at {self.gpmc_cache_path}")
            return None
        
        # Check schema on first use
        if not self.gpmc_schema_checked:
            if not self.check_gpmc_cache_schema():
                return None
        
        try:
            conn = sqlite3.connect(self.gpmc_cache_path)
            cursor = conn.cursor()
            
            # Strategy 1: Exact filename match (case-sensitive)
            cursor.execute(
                "SELECT media_key FROM remote_media WHERE file_name = ? LIMIT 1",
                (filename,)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (exact match): {filename}")
                conn.close()
                return result[0]
            
            # Strategy 2: Case-insensitive match
            cursor.execute(
                "SELECT media_key FROM remote_media WHERE LOWER(file_name) = LOWER(?) LIMIT 1",
                (filename,)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (case-insensitive): {filename}")
                conn.close()
                return result[0]
            
            # Strategy 3: Basename match (in case cache has full paths)
            # This handles cases where the cache might store "/path/to/file.mkv" but we search for "file.mkv"
            cursor.execute(
                """SELECT media_key FROM remote_media 
                   WHERE file_name LIKE '%' || ? 
                   OR LOWER(file_name) LIKE LOWER('%' || ?)
                   LIMIT 1""",
                (filename, filename)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (pattern match): {filename}")
                conn.close()
                return result[0]
            
            logger.debug(f"  Not found in gpmc cache after trying all strategies: {filename}")
            conn.close()
            return None
            
        except Exception as e:
            logger.debug(f"  Failed to search gpmc cache for {filename}: {e}")
            return None
    
    def search_media_key_via_api(self, filename: str) -> Optional[str]:
        """
        Search for a media key by querying Google Photos API directly.
        
        This is a last-resort fallback when both upload_state.json and gpmc cache fail.
        Uses the gpmc client to search the library for files matching the filename.
        
        Args:
            filename: The filename to search for
            
        Returns:
            The media_key if found, None otherwise
        """
        if not self.client:
            logger.debug(f"  Client not initialized, cannot search API")
            return None
        
        try:
            logger.info(f"   üîç Searching Google Photos API for: {filename}")
            
            # Query the library using gpmc's search functionality
            # The Client object should have access to the library cache
            # Try to find the file by updating cache and searching again
            try:
                # Force a targeted cache update for this specific file
                # This will query Google Photos API to get latest library state
                logger.debug(f"   Updating cache to find: {filename}")
                self.client.update_cache(show_progress=False)
                
                # Now search in the freshly updated cache
                media_key = self.search_media_key_in_gpmc_cache(filename)
                if media_key:
                    logger.info(f"   ‚úÖ Found via API cache refresh: {filename} -> {media_key}")
                    return media_key
                else:
                    logger.debug(f"   File still not found after cache refresh: {filename}")
                    return None
                    
            except Exception as e:
                logger.debug(f"   Failed to update cache for API search: {e}")
                return None
                
        except Exception as e:
            logger.debug(f"  Failed to search via API for {filename}: {e}")
            return None
    
    def build_file_to_album_map(self, node: Dict, album_path: str = "") -> None:
        """
        Recursively build a mapping of files to their album paths.
        
        Args:
            node: Current directory node from manifest structure
            album_path: Current album path (e.g., "Blockbuster Movies/Avatar (2009)")
        """
        # Process files in current directory
        for file_info in node.get('files', []):
            filename = file_info['name']
            
            # Map ISO to MKV (conversion happened during upload)
            target_filename = self.map_iso_to_mkv(filename)
            
            # Use album_path as the album name (folder structure)
            # Files in root (empty album_path) are skipped - they stay in main library
            if album_path:
                self.file_to_album_map[target_filename] = album_path
                logger.debug(f"  {target_filename} -> album: {album_path}")
            else:
                logger.debug(f"  {target_filename} -> (root, no album)")
        
        # Process subdirectories recursively
        for subdir in node.get('subdirectories', []):
            subdir_name = subdir['name']
            # Build album path (e.g., "Blockbuster Movies/Avatar (2009)")
            new_album_path = f"{album_path}/{subdir_name}" if album_path else subdir_name
            self.build_file_to_album_map(subdir, new_album_path)
    
    def get_files_by_album(self) -> Dict[str, List[str]]:
        """
        Group files by their target album.
        
        Returns:
            Dict mapping album name to list of filenames
        """
        album_to_files = defaultdict(list)
        
        for filename, album_path in self.file_to_album_map.items():
            album_to_files[album_path].append(filename)
        
        return dict(album_to_files)
    
    def load_media_cache_from_state(self, state_file: str = 'upload_state.json') -> None:
        """
        Load media keys from the upload state file.
        
        The upload state file contains media_key for each successfully uploaded file.
        This allows us to skip re-uploading or re-searching for files.
        
        Note: ISO files were converted to MKV during upload, so we map the keys accordingly.
        
        Args:
            state_file: Path to upload_state.json
        """
        try:
            if not os.path.exists(state_file):
                logger.warning(f"‚ö†Ô∏è Upload state file not found: {state_file}")
                logger.warning("   Will attempt to use gpmc cache as fallback")
                return
            
            with open(state_file, 'r') as f:
                state = json.load(f)
            
            # Extract completed uploads with media keys
            completed = state.get('completed', [])
            
            # If completed is a list of paths (old format), we can't get media keys
            if isinstance(completed, list):
                logger.warning("‚ö†Ô∏è Old state format (v1.0) detected - doesn't contain media keys")
                logger.warning("   The state file needs to be updated to v2.0 format with media keys")
                logger.warning("   Re-run the upload workflow to generate the new format")
                logger.warning("   Will attempt to use gpmc cache as fallback")
                return
            
            # If it's a dict mapping path -> info, extract media keys
            cached_count = 0
            skipped_none_count = 0
            
            if isinstance(completed, dict):
                for file_path, info in completed.items():
                    if isinstance(info, dict) and 'media_key' in info:
                        filename = os.path.basename(file_path)
                        media_key = info['media_key']
                        
                        # Skip if media_key is None (happens with migrated v1.0 state files)
                        if media_key is None:
                            logger.debug(f"  Skipping {filename} - media_key is None (migrated from old state format)")
                            skipped_none_count += 1
                            continue
                        
                        # Store with original filename
                        self.media_cache[filename] = media_key
                        cached_count += 1
                        
                        # If it's an ISO file, also store the MKV mapping
                        # (ISO files were converted to MKV during upload)
                        if filename.lower().endswith('.iso'):
                            mkv_filename = self.map_iso_to_mkv(filename)
                            self.media_cache[mkv_filename] = media_key
                            logger.debug(f"  Cached ISO mapping: {filename} -> {mkv_filename} -> {media_key}")
                        else:
                            logger.debug(f"  Cached: {filename} -> {media_key}")
            
            logger.info(f"‚úÖ Loaded {cached_count} media keys from state file")
            if skipped_none_count > 0:
                logger.warning(f"‚ö†Ô∏è Skipped {skipped_none_count} entries with None media_key")
                logger.warning("   These files were likely uploaded before state v2.0")
                logger.warning("   Will attempt to find them using gpmc cache fallback")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load media cache from state: {e}")
            logger.warning("   Will attempt to use gpmc cache as fallback")
    
    def organize_files(self, dry_run: bool = False, partial_albums: bool = True, 
                      min_files_threshold: int = 1) -> tuple:
        """
        Organize uploaded files into albums based on manifest structure.
        
        Args:
            dry_run: If True, only log what would be done without making changes
            partial_albums: If True, create albums even if some files are missing
            min_files_threshold: Minimum number of files required to create an album
            
        Returns:
            Tuple of (successful_count, failed_count, skipped_count)
        """
        logger.info("=" * 80)
        logger.info("üìÅ BUILDING FILE-TO-ALBUM MAPPING")
        logger.info("=" * 80)
        
        # Build the mapping from manifest structure
        self.build_file_to_album_map(self.manifest['structure'])
        
        logger.info(f"‚úÖ Mapped {len(self.file_to_album_map)} files to albums")
        
        # Group files by album
        album_to_files = self.get_files_by_album()
        
        logger.info(f"üìä Files will be organized into {len(album_to_files)} albums")
        
        # Load media cache from upload state (if available)
        self.load_media_cache_from_state()
        
        # Check gpmc cache schema early (if it exists)
        if self.gpmc_cache_path and self.gpmc_cache_path.exists():
            logger.info("=" * 80)
            logger.info("üîç CHECKING GPMC CACHE DATABASE")
            logger.info("=" * 80)
            self.check_gpmc_cache_schema()
        
        successful = 0
        failed = 0
        skipped = 0
        
        # Track missing files for final summary
        missing_files_report = []
        
        # Process each album
        for album_name, filenames in sorted(album_to_files.items()):
            logger.info("=" * 80)
            logger.info(f"üìÅ Processing album: {album_name}")
            logger.info(f"   Files to add: {len(filenames)}")
            
            if dry_run:
                logger.info(f"   [DRY RUN] Would create album: {album_name}")
                for filename in filenames[:5]:  # Show first 5
                    logger.info(f"   [DRY RUN]   - {filename}")
                if len(filenames) > 5:
                    logger.info(f"   [DRY RUN]   ... and {len(filenames) - 5} more")
                successful += len(filenames)
                continue
            
            # Collect media keys for files in this album
            media_keys = []
            not_found_count = 0
            found_in_cache_count = 0
            found_in_gpmc_count = 0
            missing_files_list = []
            
            for filename in filenames:
                media_key = None
                
                # Check upload state cache first
                if filename in self.media_cache:
                    media_key = self.media_cache[filename]
                    found_in_cache_count += 1
                    logger.debug(f"   ‚úì Found in upload state cache: {filename}")
                else:
                    # Fallback 1: search in gpmc cache database
                    media_key = self.search_media_key_in_gpmc_cache(filename)
                    if media_key:
                        found_in_gpmc_count += 1
                        logger.info(f"   ‚úì Found in gpmc cache: {filename} -> {media_key}")
                        # Cache it for future lookups
                        self.media_cache[filename] = media_key
                    else:
                        # Fallback 2: search via Google Photos API (last resort)
                        logger.info(f"   ‚ö†Ô∏è Not found in local caches, trying API lookup: {filename}")
                        media_key = self.search_media_key_via_api(filename)
                        if media_key:
                            found_in_gpmc_count += 1
                            logger.info(f"   ‚úÖ Found via API refresh: {filename} -> {media_key}")
                            # Cache it for future lookups
                            self.media_cache[filename] = media_key
                        else:
                            logger.warning(f"   ‚ö†Ô∏è Not found anywhere (state, cache, API): {filename}")
                            
                            # Provide specific diagnostic information
                            has_upload_state = os.path.exists('upload_state.json')
                            has_gpmc_cache = self.gpmc_cache_path and self.gpmc_cache_path.exists()
                            
                            if not has_upload_state and not has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: No cache sources available")
                                logger.warning(f"         - upload_state.json: Not found")
                                logger.warning(f"         - gpmc cache: Not found")
                                logger.warning(f"      üîß ACTION: Run 'gpmc update-cache' to build cache OR upload files first")
                            elif not has_upload_state and has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: File not in gpmc cache (upload_state.json not available)")
                                logger.warning(f"      üîß ACTION: File may not be uploaded yet OR run 'gpmc update-cache' to refresh cache")
                            elif has_upload_state and not has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: File not in upload_state.json (gpmc cache not available)")
                                logger.warning(f"      üîß ACTION: File may not be uploaded yet OR run 'gpmc update-cache' to build fallback cache")
                            else:
                                logger.warning(f"      üìã DIAGNOSIS: File not found in any source (state, cache, API)")
                                logger.warning(f"         - upload_state.json: No entry for {filename}")
                                logger.warning(f"         - gpmc cache: No match after trying all search strategies")
                                logger.warning(f"         - API refresh: File not found in Google Photos library")
                                logger.warning(f"      üîß POSSIBLE CAUSES:")
                                logger.warning(f"         1. File has not been uploaded to Google Photos yet")
                                logger.warning(f"         2. Filename mismatch (check for special characters, encoding issues)")
                                logger.warning(f"         3. File was uploaded but hasn't synced to Google Photos library yet")
                            
                            not_found_count += 1
                            skipped += 1
                            missing_files_list.append(filename)
                
                if media_key:
                    media_keys.append(media_key)
            
            # Calculate cache hit rate for this album
            total_files = len(filenames)
            found_files = found_in_cache_count + found_in_gpmc_count
            cache_hit_rate = (found_files / total_files * 100) if total_files > 0 else 0
            
            # Log summary for this album
            logger.info(f"   üìä ALBUM CACHE SUMMARY:")
            logger.info(f"      Total files in album: {total_files}")
            logger.info(f"      Found in upload state cache: {found_in_cache_count}")
            logger.info(f"      Found via gpmc cache fallback: {found_in_gpmc_count}")
            logger.info(f"      Missing from all caches: {not_found_count}")
            logger.info(f"      Cache hit rate: {cache_hit_rate:.1f}%")
            
            # Handle missing files based on partial_albums setting
            if not media_keys:
                logger.warning(f"   ‚ö†Ô∏è No media keys found for album: {album_name}")
                logger.warning(f"      All {total_files} files are missing from cache")
                logger.warning(f"      Skipping album (files may not be uploaded yet)")
                
                # Add to missing files report
                missing_files_report.append({
                    'album': album_name,
                    'total_files': total_files,
                    'missing_files': missing_files_list,
                    'status': 'SKIPPED - No files found'
                })
                continue
            elif not_found_count > 0:
                if partial_albums and len(media_keys) >= min_files_threshold:
                    logger.warning(f"   ‚ö†Ô∏è Partial album: {not_found_count}/{total_files} files missing")
                    logger.warning(f"      Creating album with {len(media_keys)} available files")
                    logger.warning(f"      Missing files can be added later when available:")
                    for missing_file in missing_files_list[:10]:  # Show first 10
                        logger.warning(f"         - {missing_file}")
                    if len(missing_files_list) > 10:
                        logger.warning(f"         ... and {len(missing_files_list) - 10} more")
                    
                    # Add to missing files report
                    missing_files_report.append({
                        'album': album_name,
                        'total_files': total_files,
                        'found_files': len(media_keys),
                        'missing_files': missing_files_list,
                        'status': 'PARTIAL - Some files added'
                    })
                else:
                    logger.warning(f"   ‚ö†Ô∏è Too many files missing: {not_found_count}/{total_files}")
                    logger.warning(f"      Only {len(media_keys)} files found (threshold: {min_files_threshold})")
                    logger.warning(f"      Skipping album - enable partial_albums or lower threshold to proceed")
                    
                    # Add to missing files report
                    missing_files_report.append({
                        'album': album_name,
                        'total_files': total_files,
                        'found_files': len(media_keys),
                        'missing_files': missing_files_list,
                        'status': f'SKIPPED - Below threshold ({min_files_threshold})'
                    })
                    continue
            
            # Add media to album
            try:
                logger.info(f"   üì§ Adding {len(media_keys)} files to album: {album_name}")
                
                result = self.client.add_to_album(
                    media_keys=media_keys,
                    album_name=album_name,
                    show_progress=True
                )
                
                if result:
                    logger.info(f"   ‚úÖ Successfully added {len(media_keys)} files to album")
                    successful += len(media_keys)
                else:
                    logger.error(f"   ‚ùå Failed to add files to album")
                    logger.error(f"      Result from API: {result}")
                    logger.error(f"      This may indicate an API error or authentication issue")
                    failed += len(media_keys)
                    
            except Exception as e:
                logger.error(f"   ‚ùå Error adding files to album: {e}")
                failed += len(media_keys)
        
        # Generate detailed missing files report
        if missing_files_report:
            logger.info("=" * 80)
            logger.info("üìã MISSING FILES REPORT")
            logger.info("=" * 80)
            logger.info(f"Albums with missing files: {len(missing_files_report)}")
            logger.info("")
            
            for report in missing_files_report:
                logger.info(f"Album: {report['album']}")
                logger.info(f"  Status: {report['status']}")
                logger.info(f"  Total files: {report['total_files']}")
                if 'found_files' in report:
                    logger.info(f"  Found files: {report['found_files']}")
                logger.info(f"  Missing files ({len(report['missing_files'])}):")
                for missing_file in report['missing_files'][:5]:  # Show first 5
                    logger.info(f"    - {missing_file}")
                if len(report['missing_files']) > 5:
                    logger.info(f"    ... and {len(report['missing_files']) - 5} more")
                logger.info("")
            
            logger.info("=" * 80)
            logger.info("üîß RECOMMENDED ACTIONS")
            logger.info("=" * 80)
            logger.info("To resolve missing files:")
            logger.info("  1. Run 'gpmc update-cache' to refresh the Google Photos cache")
            logger.info("  2. Verify files were successfully uploaded (check upload_state.json)")
            logger.info("  3. Re-run this organization script to pick up newly found files")
            logger.info("  4. Check for filename mismatches (special characters, encoding)")
            logger.info("=" * 80)
        
        return successful, failed, skipped


def main():
    """Main function."""
    logger.info("=" * 80)
    logger.info("GOOGLE PHOTOS FILE ORGANIZER")
    logger.info("Organize uploaded files into albums based on FTP structure")
    logger.info("=" * 80)
    
    # Check command line arguments
    dry_run = '--dry-run' in sys.argv
    partial_albums = '--no-partial-albums' not in sys.argv  # Default: enabled
    
    # Parse min files threshold if provided
    min_files_threshold = 1
    for arg in sys.argv:
        if arg.startswith('--min-files='):
            try:
                min_files_threshold = int(arg.split('=')[1])
            except ValueError:
                logger.warning(f"Invalid --min-files value: {arg}, using default of 1")
    
    if dry_run:
        logger.info("üîç DRY RUN MODE - No changes will be made")
        logger.info("=" * 80)
    
    logger.info(f"‚öôÔ∏è Configuration:")
    logger.info(f"   Partial albums: {'Enabled' if partial_albums else 'Disabled'}")
    logger.info(f"   Minimum files threshold: {min_files_threshold}")
    logger.info("=" * 80)
    
    # Get auth data from environment
    auth_data = os.environ.get('GP_AUTH_DATA')
    if not auth_data:
        logger.error("‚ùå GP_AUTH_DATA environment variable not set!")
        logger.error("   Set it with: export GP_AUTH_DATA='your_auth_data'")
        sys.exit(1)
    
    logger.info(f"‚úÖ Auth data found ({len(auth_data)} characters)")
    
    # Find manifest file
    manifest_path = 'ftp_structure_manifest.json'
    
    # Check if manifest exists
    if not os.path.exists(manifest_path):
        logger.error(f"‚ùå Manifest file not found: {manifest_path}")
        logger.error("   Please download it from GitHub Actions artifacts:")
        logger.error("   1. Go to Actions -> Generate FTP Structure Tree")
        logger.error("   2. Download the 'ftp-structure-manifest' artifact")
        logger.error("   3. Extract ftp_structure_manifest.json to this directory")
        sys.exit(1)
    
    # Initialize organizer
    organizer = PhotoOrganizer(auth_data, manifest_path)
    
    # Load manifest
    if not organizer.load_manifest():
        logger.error("‚ùå Failed to load manifest")
        sys.exit(1)
    
    # Initialize Google Photos client
    if not organizer.initialize_client():
        logger.error("‚ùå Failed to initialize Google Photos client")
        sys.exit(1)
    
    # Organize files
    logger.info("=" * 80)
    logger.info("üöÄ STARTING ORGANIZATION")
    logger.info("=" * 80)
    
    try:
        successful, failed, skipped = organizer.organize_files(
            dry_run=dry_run,
            partial_albums=partial_albums,
            min_files_threshold=min_files_threshold
        )
        
        logger.info("=" * 80)
        logger.info("üìä ORGANIZATION COMPLETE")
        logger.info("=" * 80)
        logger.info(f"‚úÖ Successful: {successful}")
        logger.info(f"‚ùå Failed: {failed}")
        logger.info(f"‚è≠Ô∏è Skipped: {skipped}")
        logger.info("=" * 80)
        
        if failed > 0:
            logger.warning("‚ö†Ô∏è Some files failed to organize - check logs for details")
            sys.exit(1)
        
        if dry_run:
            logger.info("üîç DRY RUN COMPLETE - No changes were made")
            logger.info("   Run without --dry-run to actually organize files")
        else:
            logger.info("‚úÖ Files have been organized into albums!")
            logger.info("   Check Google Photos to see your organized library")
            if skipped > 0:
                logger.info("")
                logger.info("‚ö†Ô∏è Note: Some files were skipped - see MISSING FILES REPORT above")
                logger.info("   These files can be added later by re-running this script")
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == '__main__':
    main()
