#!/usr/bin/env python3
"""
Organize already-uploaded Google Photos files into albums based on FTP manifest.

This script:
1. Loads the FTP structure manifest (JSON) generated by generate_ftp_tree.py
2. Maps ISO files to MKV equivalents (since .iso files were converted during upload)
3. Creates albums using the leaf folder names from the FTP structure
4. Moves uploaded files into their corresponding albums

The script handles the fact that all files were initially uploaded to the main
Google Photos library without folder structure, and now need to be organized.

Album names use the full path (e.g., "Blockbuster Movies/Avatar (2009)")
to preserve folder hierarchy in a flat naming scheme, since Google Photos doesn't support nested folders.
"""

import os
import sys
import json
import logging
import hashlib
import sqlite3
import time
from pathlib import Path
from typing import List, Dict, Optional, Set
from collections import defaultdict

try:
    from gpmc import Client
except ImportError:
    print("ERROR: gpmc library not installed. Install with: pip install https://github.com/xob0t/google_photos_mobile_client/archive/refs/heads/main.zip")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('organize_gphotos.log')
    ]
)
logger = logging.getLogger(__name__)


class PhotoOrganizer:
    """Organizes Google Photos files based on FTP manifest structure."""
    
    def __init__(self, auth_data: str, manifest_path: str):
        """
        Initialize the organizer.
        
        Args:
            auth_data: Google Photos authentication data
            manifest_path: Path to FTP structure manifest JSON file
        """
        self.auth_data = auth_data
        self.manifest_path = manifest_path
        self.client = None
        self.manifest = None
        self.file_to_album_map = {}
        self.media_cache = {}  # Cache of filename -> media_key
        self.gpmc_cache_path = None  # Path to gpmc cache database
        self.gpmc_schema_checked = False  # Track if we've validated the schema
        
    def load_manifest(self) -> bool:
        """Load the FTP structure manifest from JSON file."""
        try:
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                self.manifest = json.load(f)
            
            logger.info(f"‚úÖ Loaded manifest from {self.manifest_path}")
            logger.info(f"   Server: {self.manifest['metadata']['server']}")
            logger.info(f"   Total files: {self.manifest['statistics']['total_files']}")
            logger.info(f"   Total size: {self.manifest['statistics']['total_size_gb']} GB")
            return True
        except FileNotFoundError:
            logger.error(f"‚ùå Manifest file not found: {self.manifest_path}")
            return False
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Invalid JSON in manifest file: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error loading manifest: {e}")
            return False
    
    def initialize_client(self) -> bool:
        """Initialize Google Photos client."""
        try:
            self.client = Client(auth_data=self.auth_data)
            logger.info("‚úÖ Google Photos client initialized")
            
            # Get the actual cache path from the client
            # gpmc stores cache at ~/.gpmc/{email}/storage.db
            self.gpmc_cache_path = self.client.db_path
            
            # Detailed diagnostics for cache location
            cache_dir = self.gpmc_cache_path.parent
            logger.info(f"üîç Checking for cache at: {self.gpmc_cache_path}")
            logger.info(f"   Cache directory: {cache_dir}")
            logger.info(f"   Cache dir exists: {cache_dir.exists()}")
            
            if cache_dir.exists():
                # List files in cache directory
                cache_files = list(cache_dir.iterdir())
                logger.info(f"   Files in cache dir: {[f.name for f in cache_files]}")
            
            if self.gpmc_cache_path.exists():
                cache_size = self.gpmc_cache_path.stat().st_size
                logger.info(f"‚úÖ Found gpmc cache database at {self.gpmc_cache_path}")
                logger.info(f"   Cache size: {cache_size / 1024 / 1024:.2f} MB")
            else:
                logger.warning(f"‚ö†Ô∏è gpmc cache database not found at {self.gpmc_cache_path}")
                logger.warning("   The cache should have been built by the workflow")
                logger.warning("   Check the 'Update Google Photos cache' step logs")
                
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Google Photos client: {e}")
            return False
    
    def map_iso_to_mkv(self, filename: str) -> str:
        """
        Map an ISO filename to its MKV equivalent.
        
        During upload, .iso files were converted to .mkv with same base name.
        
        Args:
            filename: Original filename (e.g., "Movie.iso")
            
        Returns:
            MKV filename (e.g., "Movie.mkv")
        """
        if filename.lower().endswith('.iso'):
            return filename[:-4] + '.mkv'
        return filename
    
    def check_gpmc_cache_schema(self) -> bool:
        """
        Check if the gpmc cache database has the expected schema.
        Logs helpful information if the schema is different.
        
        Returns:
            True if schema looks correct, False otherwise
        """
        if self.gpmc_schema_checked:
            return True
            
        if not self.gpmc_cache_path or not self.gpmc_cache_path.exists():
            logger.warning("‚ö†Ô∏è Cannot check schema - cache file doesn't exist")
            logger.warning(f"   Expected location: {self.gpmc_cache_path}")
            return False
        
        try:
            conn = sqlite3.connect(self.gpmc_cache_path)
            cursor = conn.cursor()
            
            # Check if remote_media table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='remote_media'"
            )
            if not cursor.fetchone():
                logger.warning("‚ö†Ô∏è gpmc cache database doesn't have 'remote_media' table")
                logger.warning("   Run 'gpmc update-cache' to build the cache properly")
                conn.close()
                return False
            
            # Check the schema of remote_media table
            cursor.execute("PRAGMA table_info(remote_media)")
            columns = cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            logger.debug(f"  gpmc cache columns: {column_names}")
            
            # Check for required columns
            if 'file_name' not in column_names:
                logger.warning("‚ö†Ô∏è gpmc cache 'remote_media' table missing 'file_name' column")
                logger.warning(f"   Available columns: {column_names}")
                logger.warning("   Run 'gpmc update-cache' to rebuild the cache")
                conn.close()
                return False
            
            if 'media_key' not in column_names:
                logger.warning("‚ö†Ô∏è gpmc cache 'remote_media' table missing 'media_key' column")
                logger.warning(f"   Available columns: {column_names}")
                logger.warning("   Run 'gpmc update-cache' to rebuild the cache")
                conn.close()
                return False
            
            # Check if there's any data
            cursor.execute("SELECT COUNT(*) FROM remote_media")
            count = cursor.fetchone()[0]
            logger.info(f"‚úÖ gpmc cache has {count} media items")
            
            if count == 0:
                logger.warning("‚ö†Ô∏è gpmc cache is empty - run 'gpmc update-cache' to populate it")
            else:
                # Show sample filenames to help with debugging
                logger.info("üìÅ Sample filenames from cache (first 10):")
                cursor.execute("SELECT file_name FROM remote_media LIMIT 10")
                for (filename,) in cursor.fetchall():
                    logger.info(f"   - {filename}")
            
            conn.close()
            self.gpmc_schema_checked = True
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to check gpmc cache schema: {e}")
            return False
    
    def search_media_key_in_gpmc_cache(self, filename: str) -> Optional[str]:
        """
        Search for a media key by filename in the gpmc local cache database.
        
        This serves as a fallback when the upload_state.json doesn't have the media key.
        Uses multiple search strategies to find the file:
        1. Exact filename match (case-sensitive)
        2. Case-insensitive filename match
        3. Basename match (filename only, no path)
        4. LIKE pattern match for files ending with the filename
        
        Args:
            filename: The filename to search for
            
        Returns:
            The media_key if found, None otherwise
        """
        if not self.gpmc_cache_path or not self.gpmc_cache_path.exists():
            logger.debug(f"  gpmc cache not available at {self.gpmc_cache_path}")
            return None
        
        # Check schema on first use
        if not self.gpmc_schema_checked:
            if not self.check_gpmc_cache_schema():
                return None
        
        try:
            conn = sqlite3.connect(self.gpmc_cache_path)
            cursor = conn.cursor()
            
            # Strategy 1: Exact filename match (case-sensitive)
            cursor.execute(
                "SELECT media_key FROM remote_media WHERE file_name = ? LIMIT 1",
                (filename,)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (exact match): {filename}")
                conn.close()
                return result[0]
            
            # Strategy 2: Case-insensitive match
            cursor.execute(
                "SELECT media_key FROM remote_media WHERE LOWER(file_name) = LOWER(?) LIMIT 1",
                (filename,)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (case-insensitive): {filename}")
                conn.close()
                return result[0]
            
            # Strategy 3: Basename match (in case cache has full paths)
            # This handles cases where the cache might store "/path/to/file.mkv" but we search for "file.mkv"
            cursor.execute(
                """SELECT media_key FROM remote_media 
                   WHERE file_name LIKE '%' || ? 
                   OR LOWER(file_name) LIKE LOWER('%' || ?)
                   LIMIT 1""",
                (filename, filename)
            )
            result = cursor.fetchone()
            if result:
                logger.debug(f"  Found in gpmc cache (pattern match): {filename}")
                conn.close()
                return result[0]
            
            logger.debug(f"  Not found in gpmc cache after trying all strategies: {filename}")
            conn.close()
            return None
            
        except Exception as e:
            logger.debug(f"  Failed to search gpmc cache for {filename}: {e}")
            return None
    
    def search_media_key_via_api(self, filename: str) -> Optional[str]:
        """
        Search for a media key by doing an exhaustive search in the gpmc cache database.
        
        This is a last-resort fallback when upload_state.json doesn't have the file.
        The gpmc cache should already be populated by the workflow's "Update Google Photos cache" step.
        
        NOTE: We do NOT call update_cache() here because:
        - The cache was already initialized/updated in the workflow before this script runs
        - update_cache() only does incremental updates (new changes since last state token)
        - Files that were already uploaded won't appear in incremental updates
        - Therefore, calling update_cache() here won't help find already-uploaded files
        
        Args:
            filename: The filename to search for
            
        Returns:
            The media_key if found, None otherwise
        """
        if not self.gpmc_cache_path or not self.gpmc_cache_path.exists():
            logger.debug(f"  gpmc cache not available, cannot search API")
            return None
        
        if not self.gpmc_schema_checked:
            if not self.check_gpmc_cache_schema():
                return None
        
        try:
            logger.info(f"   üîç Searching gpmc cache database for: {filename}")
            
            # Do a more exhaustive search in the cache database
            # Try multiple search strategies to find files with similar names
            # Use context manager to ensure connection is properly closed
            with sqlite3.connect(self.gpmc_cache_path) as conn:
                cursor = conn.cursor()
                
                # Strategy 1: Exact match (case-sensitive)
                cursor.execute(
                    "SELECT media_key, file_name FROM remote_media WHERE file_name = ? LIMIT 1",
                    (filename,)
                )
                result = cursor.fetchone()
                if result:
                    logger.info(f"   ‚úÖ Found exact match: {filename} -> {result[0]}")
                    return result[0]
                
                # Strategy 2: Case-insensitive match
                cursor.execute(
                    "SELECT media_key, file_name FROM remote_media WHERE LOWER(file_name) = LOWER(?) LIMIT 1",
                    (filename,)
                )
                result = cursor.fetchone()
                if result:
                    logger.info(f"   ‚úÖ Found case-insensitive match: {result[1]} -> {result[0]}")
                    return result[0]
                
                # Strategy 3: LIKE pattern for files ending with this filename (handles path variations)
                cursor.execute(
                    "SELECT media_key, file_name FROM remote_media WHERE file_name LIKE ? LIMIT 1",
                    (f"%{filename}",)
                )
                result = cursor.fetchone()
                if result:
                    logger.info(f"   ‚úÖ Found pattern match: {result[1]} -> {result[0]}")
                    return result[0]
                
                # Strategy 4: Case-insensitive LIKE for maximum coverage
                cursor.execute(
                    "SELECT media_key, file_name FROM remote_media WHERE LOWER(file_name) LIKE LOWER(?) LIMIT 1",
                    (f"%{filename}",)
                )
                result = cursor.fetchone()
                if result:
                    logger.info(f"   ‚úÖ Found case-insensitive pattern match: {result[1]} -> {result[0]}")
                    return result[0]
                
                # Strategy 5: Try basename matching (in case stored with different path)
                # Extract just the base filename without path (cross-platform)
                base_filename = os.path.basename(filename)
                if base_filename != filename:
                    # Use single case-insensitive LIKE query for efficiency
                    # Note: Google Photos may store paths with forward slashes regardless of OS
                    # So we check for both forward slash and backslash
                    cursor.execute(
                        "SELECT media_key, file_name FROM remote_media WHERE LOWER(file_name) LIKE LOWER(?) OR LOWER(file_name) LIKE LOWER(?) LIMIT 1",
                        (f"%/{base_filename}", f"%\\{base_filename}")
                    )
                    result = cursor.fetchone()
                    if result:
                        logger.info(f"   ‚úÖ Found basename match: {result[1]} -> {result[0]}")
                        return result[0]
                
                logger.debug(f"   File not found in cache after exhaustive search: {filename}")
                return None
                
        except Exception as e:
            logger.debug(f"  Failed to search cache database for {filename}: {e}")
            return None
    
    def build_file_to_album_map(self, node: Dict, album_path: str = "") -> None:
        """
        Recursively build a mapping of files to their album names.
        
        Album names use the FULL path to preserve folder hierarchy in a flat structure.
        For example, "Blockbuster Movies/Avatar (2009)" becomes album "Blockbuster Movies/Avatar (2009)".
        
        Args:
            node: Current directory node from manifest structure
            album_path: Current album path (e.g., "Blockbuster Movies/Avatar (2009)")
        """
        # Process files in current directory
        for file_info in node.get('files', []):
            filename = file_info['name']
            
            # Map ISO to MKV (conversion happened during upload)
            target_filename = self.map_iso_to_mkv(filename)
            
            # Use the FULL path as the album name to preserve folder hierarchy
            # Files in root (empty album_path) are skipped - they stay in main library
            if album_path:
                # Use full path for flat folder hierarchy (Google Photos doesn't support nested folders)
                album_name = album_path  # Use full path for flat folder hierarchy
                self.file_to_album_map[target_filename] = album_name
                logger.debug(f"  {target_filename} -> album: {album_name} (from path: {album_path})")
            else:
                logger.debug(f"  {target_filename} -> (root, no album)")
        
        # Process subdirectories recursively
        for subdir in node.get('subdirectories', []):
            subdir_name = subdir['name']
            # Build album path (e.g., "Blockbuster Movies/Avatar (2009)")
            new_album_path = f"{album_path}/{subdir_name}" if album_path else subdir_name
            self.build_file_to_album_map(subdir, new_album_path)
    
    def get_files_by_album(self) -> Dict[str, List[str]]:
        """
        Group files by their target album.
        
        Returns:
            Dict mapping album name to list of filenames
        """
        album_to_files = defaultdict(list)
        
        for filename, album_path in self.file_to_album_map.items():
            album_to_files[album_path].append(filename)
        
        return dict(album_to_files)
    
    def load_media_cache_from_state(self, state_file: str = 'upload_state.json') -> None:
        """
        Load media keys from the upload state file.
        
        The upload state file contains media_key for each successfully uploaded file.
        This allows us to skip re-uploading or re-searching for files.
        
        Note: ISO files were converted to MKV during upload, so we map the keys accordingly.
        
        Args:
            state_file: Path to upload_state.json
        """
        try:
            if not os.path.exists(state_file):
                logger.warning(f"‚ö†Ô∏è Upload state file not found: {state_file}")
                logger.warning("   Will attempt to use gpmc cache as fallback")
                return
            
            with open(state_file, 'r') as f:
                state = json.load(f)
            
            # Extract completed uploads with media keys
            completed = state.get('completed', [])
            
            # If completed is a list of paths (old format), we can't get media keys
            if isinstance(completed, list):
                logger.warning("‚ö†Ô∏è Old state format (v1.0) detected - doesn't contain media keys")
                logger.warning("   The state file needs to be updated to v2.0 format with media keys")
                logger.warning("   Re-run the upload workflow to generate the new format")
                logger.warning("   Will attempt to use gpmc cache as fallback")
                return
            
            # If it's a dict mapping path -> info, extract media keys
            cached_count = 0
            skipped_none_count = 0
            migrated_count = 0
            recovered_keys = {}  # Track files with None keys that we can potentially recover
            
            if isinstance(completed, dict):
                for file_path, info in completed.items():
                    if isinstance(info, dict) and 'media_key' in info:
                        filename = os.path.basename(file_path)
                        media_key = info['media_key']
                        
                        # Skip if media_key is None (happens with migrated v1.0 state files)
                        if media_key is None:
                            logger.debug(f"  Skipping {filename} - media_key is None (migrated from old state format)")
                            skipped_none_count += 1
                            migrated_count += 1
                            # Try to recover the media key from cache
                            recovered_key = self.search_media_key_in_gpmc_cache(filename)
                            if recovered_key:
                                recovered_keys[file_path] = recovered_key
                                # Store in our memory cache
                                self.media_cache[filename] = recovered_key
                                cached_count += 1
                                logger.info(f"  ‚úÖ Recovered media key for {filename} from cache")
                            continue
                        
                        # Store with original filename
                        self.media_cache[filename] = media_key
                        cached_count += 1
                        
                        # If it's an ISO file, also store the MKV mapping
                        # (ISO files were converted to MKV during upload)
                        if filename.lower().endswith('.iso'):
                            mkv_filename = self.map_iso_to_mkv(filename)
                            self.media_cache[mkv_filename] = media_key
                            logger.debug(f"  Cached ISO mapping: {filename} -> {mkv_filename} -> {media_key}")
                        else:
                            logger.debug(f"  Cached: {filename} -> {media_key}")
            
            logger.info(f"‚úÖ Loaded {cached_count} media keys from state file")
            if len(recovered_keys) > 0:
                logger.info(f"‚úÖ Recovered {len(recovered_keys)} media keys from cache for migrated files")
            if skipped_none_count > len(recovered_keys):
                remaining_none = skipped_none_count - len(recovered_keys)
                logger.warning(f"‚ö†Ô∏è Still have {remaining_none} entries with None media_key (couldn't recover)")
                logger.warning(f"   These files were uploaded before state v2.0 and not found in cache")
                logger.warning(f"   Will attempt to find them using API fallback during organization")
                logger.warning(f"   üí° TIP: To fix this permanently, re-upload these files or run cache update")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load media cache from state: {e}")
            logger.warning("   Will attempt to use gpmc cache as fallback")
    
    def organize_files(self, dry_run: bool = False, partial_albums: bool = True, 
                      min_files_threshold: int = 1) -> tuple:
        """
        Organize uploaded files into albums based on manifest structure.
        
        Args:
            dry_run: If True, only log what would be done without making changes
            partial_albums: If True, create albums even if some files are missing
            min_files_threshold: Minimum number of files required to create an album
            
        Returns:
            Tuple of (successful_count, failed_count, skipped_count)
        """
        logger.info("=" * 80)
        logger.info("üìÅ BUILDING FILE-TO-ALBUM MAPPING")
        logger.info("=" * 80)
        
        # Build the mapping from manifest structure
        self.build_file_to_album_map(self.manifest['structure'])
        
        logger.info(f"‚úÖ Mapped {len(self.file_to_album_map)} files to albums")
        
        # Group files by album
        album_to_files = self.get_files_by_album()
        
        logger.info(f"üìä Files will be organized into {len(album_to_files)} albums")
        
        # Load media cache from upload state (if available)
        self.load_media_cache_from_state()
        
        # Check gpmc cache schema early (if it exists)
        if self.gpmc_cache_path and self.gpmc_cache_path.exists():
            logger.info("=" * 80)
            logger.info("üîç CHECKING GPMC CACHE DATABASE")
            logger.info("=" * 80)
            self.check_gpmc_cache_schema()
        
        successful = 0
        failed = 0
        skipped = 0
        
        # Track statistics for lookup methods used
        lookup_stats = {
            'from_state': 0,
            'from_cache': 0,
            'from_exhaustive_search': 0,
            'not_found': 0
        }
        
        # Track missing files for final summary
        missing_files_report = []
        
        # Process each album
        for album_name, filenames in sorted(album_to_files.items()):
            logger.info("=" * 80)
            logger.info(f"üìÅ Processing album: {album_name}")
            logger.info(f"   Files to add: {len(filenames)}")
            
            # Collect media keys for files in this album (do this even in dry-run to check availability)
            media_keys = []
            not_found_count = 0
            found_in_cache_count = 0
            found_in_gpmc_count = 0
            missing_files_list = []
            
            for filename in filenames:
                media_key = None
                lookup_method = None
                
                # Check upload state cache first
                if filename in self.media_cache:
                    media_key = self.media_cache[filename]
                    found_in_cache_count += 1
                    lookup_method = 'from_state'
                    logger.debug(f"   ‚úì Found in upload state cache: {filename}")
                else:
                    # Fallback 1: search in gpmc cache database
                    media_key = self.search_media_key_in_gpmc_cache(filename)
                    if media_key:
                        found_in_gpmc_count += 1
                        lookup_method = 'from_cache'
                        logger.info(f"   ‚úì Found in gpmc cache: {filename} -> {media_key}")
                        # Cache it for future lookups
                        self.media_cache[filename] = media_key
                    else:
                        # Fallback 2: do exhaustive search in gpmc cache database (last resort)
                        logger.info(f"   ‚ö†Ô∏è Not found in initial cache lookup, trying exhaustive search: {filename}")
                        media_key = self.search_media_key_via_api(filename)
                        if media_key:
                            found_in_gpmc_count += 1
                            lookup_method = 'from_exhaustive_search'
                            # Cache it for future lookups
                            self.media_cache[filename] = media_key
                        else:
                            lookup_method = 'not_found'
                            logger.warning(f"   ‚ö†Ô∏è Not found anywhere (state, cache, API): {filename}")
                            
                            # Provide specific diagnostic information
                            has_upload_state = os.path.exists('upload_state.json')
                            has_gpmc_cache = self.gpmc_cache_path and self.gpmc_cache_path.exists()
                            
                            if not has_upload_state and not has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: No cache sources available")
                                logger.warning(f"         - upload_state.json: Not found")
                                logger.warning(f"         - gpmc cache: Not found")
                                logger.warning(f"      üîß ACTION: The workflow should have built the cache - check logs")
                            elif not has_upload_state and has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: File not in gpmc cache (upload_state.json not available)")
                                logger.warning(f"      üîß ACTION: File may not be uploaded yet")
                                logger.warning(f"      üí° NOTE: The cache should be rebuilt on each workflow run for full library sync")
                            elif has_upload_state and not has_gpmc_cache:
                                logger.warning(f"      üìã DIAGNOSIS: File not in upload_state.json (gpmc cache not available)")
                                logger.warning(f"      üîß ACTION: The workflow should have built the cache - check logs")
                            else:
                                logger.warning(f"      üìã DIAGNOSIS: File not found in any source (state, cache, API)")
                                logger.warning(f"         - upload_state.json: No entry for {filename}")
                                logger.warning(f"         - gpmc cache: No match after trying all search strategies")
                                logger.warning(f"         - Exhaustive cache search: File not found in Google Photos library")
                                logger.warning(f"      üîß POSSIBLE CAUSES:")
                                logger.warning(f"         1. File has not been uploaded to Google Photos yet")
                                logger.warning(f"         2. Filename mismatch (check for special characters, encoding issues)")
                                logger.warning(f"         3. File was uploaded but Google Photos API hasn't indexed it yet")
                                logger.warning(f"         4. The cache may have been built before files were uploaded")
                                logger.warning(f"      üí° TIP: The workflow rebuilds the cache on each run to avoid this issue")
                            
                            not_found_count += 1
                            skipped += 1
                            missing_files_list.append(filename)
                
                # Track which lookup method was used
                if lookup_method:
                    lookup_stats[lookup_method] += 1
                
                if media_key:
                    media_keys.append(media_key)
            
            # Calculate cache hit rate for this album
            total_files = len(filenames)
            found_files = found_in_cache_count + found_in_gpmc_count
            cache_hit_rate = (found_files / total_files * 100) if total_files > 0 else 0
            
            # Log summary for this album
            logger.info(f"   üìä ALBUM CACHE SUMMARY:")
            logger.info(f"      Total files in album: {total_files}")
            logger.info(f"      Found in upload state cache: {found_in_cache_count}")
            logger.info(f"      Found via gpmc cache fallback: {found_in_gpmc_count}")
            logger.info(f"      Missing from all caches: {not_found_count}")
            logger.info(f"      Cache hit rate: {cache_hit_rate:.1f}%")
            
            # Handle missing files based on partial_albums setting
            if not media_keys:
                logger.warning(f"   ‚ö†Ô∏è No media keys found for album: {album_name}")
                logger.warning(f"      All {total_files} files are missing from cache")
                logger.warning(f"      Skipping album (files may not be uploaded yet)")
                
                # Add to missing files report
                missing_files_report.append({
                    'album': album_name,
                    'total_files': total_files,
                    'missing_files': missing_files_list,
                    'status': 'SKIPPED - No files found'
                })
                continue
            elif not_found_count > 0:
                if partial_albums and len(media_keys) >= min_files_threshold:
                    logger.warning(f"   ‚ö†Ô∏è Partial album: {not_found_count}/{total_files} files missing")
                    logger.warning(f"      Creating album with {len(media_keys)} available files")
                    logger.warning(f"      Missing files can be added later when available:")
                    for missing_file in missing_files_list[:10]:  # Show first 10
                        logger.warning(f"         - {missing_file}")
                    if len(missing_files_list) > 10:
                        logger.warning(f"         ... and {len(missing_files_list) - 10} more")
                    
                    # Add to missing files report
                    missing_files_report.append({
                        'album': album_name,
                        'total_files': total_files,
                        'found_files': len(media_keys),
                        'missing_files': missing_files_list,
                        'status': 'PARTIAL - Some files added'
                    })
                else:
                    logger.warning(f"   ‚ö†Ô∏è Too many files missing: {not_found_count}/{total_files}")
                    logger.warning(f"      Only {len(media_keys)} files found (threshold: {min_files_threshold})")
                    logger.warning(f"      Skipping album - enable partial_albums or lower threshold to proceed")
                    
                    # Add to missing files report
                    missing_files_report.append({
                        'album': album_name,
                        'total_files': total_files,
                        'found_files': len(media_keys),
                        'missing_files': missing_files_list,
                        'status': f'SKIPPED - Below threshold ({min_files_threshold})'
                    })
                    continue
            
            # Dry-run mode: show what would be created
            if dry_run:
                logger.info(f"   [DRY RUN] Would create album: {album_name}")
                logger.info(f"   [DRY RUN] Would add {len(media_keys)} files (out of {len(filenames)} total)")
                if not_found_count > 0:
                    logger.warning(f"   [DRY RUN] ‚ö†Ô∏è Note: {not_found_count} files missing media keys - won't be added")
                    logger.warning(f"   [DRY RUN] Missing files:")
                    for missing_file in missing_files_list[:5]:  # Show first 5 missing
                        logger.warning(f"   [DRY RUN]   ‚úó {missing_file}")
                    if len(missing_files_list) > 5:
                        logger.warning(f"   [DRY RUN]   ... and {len(missing_files_list) - 5} more missing")
                successful += len(media_keys)
                continue
            
            # Add media to album
            try:
                logger.info(f"   üì§ Adding {len(media_keys)} files to album: {album_name}")
                
                result = self.client.add_to_album(
                    media_keys=media_keys,
                    album_name=album_name,
                    show_progress=True
                )
                
                if result:
                    logger.info(f"   ‚úÖ Successfully added {len(media_keys)} files to album")
                    successful += len(media_keys)
                else:
                    logger.error(f"   ‚ùå Failed to add files to album")
                    logger.error(f"      Result from API: {result}")
                    logger.error(f"      This may indicate an API error or authentication issue")
                    failed += len(media_keys)
                    
            except Exception as e:
                logger.error(f"   ‚ùå Error adding files to album: {e}")
                failed += len(media_keys)
        
        # Generate detailed missing files report
        if missing_files_report:
            logger.info("=" * 80)
            logger.info("üìã MISSING FILES REPORT")
            logger.info("=" * 80)
            logger.info(f"Albums with missing files: {len(missing_files_report)}")
            logger.info("")
            
            for report in missing_files_report:
                logger.info(f"Album: {report['album']}")
                logger.info(f"  Status: {report['status']}")
                logger.info(f"  Total files: {report['total_files']}")
                if 'found_files' in report:
                    logger.info(f"  Found files: {report['found_files']}")
                logger.info(f"  Missing files ({len(report['missing_files'])}):")
                for missing_file in report['missing_files'][:5]:  # Show first 5
                    logger.info(f"    - {missing_file}")
                if len(report['missing_files']) > 5:
                    logger.info(f"    ... and {len(report['missing_files']) - 5} more")
                logger.info("")
            
            logger.info("=" * 80)
            logger.info("üîß RECOMMENDED ACTIONS")
            logger.info("=" * 80)
            logger.info("To resolve missing files:")
            logger.info("  1. Verify files were successfully uploaded (check upload_state.json)")
            logger.info("  2. Re-run this workflow - the cache is rebuilt on each run")
            logger.info("  3. If still missing, verify files exist in Google Photos web/app")
            logger.info("  4. Check for filename mismatches (special characters, encoding)")
            logger.info("")
            logger.info("üí° NOTE: The workflow automatically rebuilds the cache each run")
            logger.info("   to ensure all uploaded files are found, even old ones.")
            logger.info("=" * 80)
        
        # Log overall lookup statistics
        logger.info("=" * 80)
        logger.info("üìä MEDIA KEY LOOKUP STATISTICS")
        logger.info("=" * 80)
        logger.info(f"Total files processed: {sum(lookup_stats.values())}")
        logger.info(f"  - Found in upload state: {lookup_stats['from_state']}")
        logger.info(f"  - Found via gpmc cache: {lookup_stats['from_cache']}")
        logger.info(f"  - Found via exhaustive cache search: {lookup_stats['from_exhaustive_search']}")
        logger.info(f"  - Not found anywhere: {lookup_stats['not_found']}")
        
        if lookup_stats['not_found'] > 0:
            logger.warning(f"‚ö†Ô∏è  {lookup_stats['not_found']} files could not be located")
            logger.warning("   These files may need to be re-uploaded or manually organized")
        
        logger.info("=" * 80)
        
        return successful, failed, skipped


def main():
    """Main function."""
    logger.info("=" * 80)
    logger.info("GOOGLE PHOTOS FILE ORGANIZER")
    logger.info("Organize uploaded files into albums based on FTP structure")
    logger.info("=" * 80)
    
    # Check command line arguments
    dry_run = '--dry-run' in sys.argv
    partial_albums = '--no-partial-albums' not in sys.argv  # Default: enabled
    
    # Parse min files threshold if provided
    min_files_threshold = 1
    for arg in sys.argv:
        if arg.startswith('--min-files='):
            try:
                min_files_threshold = int(arg.split('=')[1])
            except ValueError:
                logger.warning(f"Invalid --min-files value: {arg}, using default of 1")
    
    if dry_run:
        logger.info("üîç DRY RUN MODE - No changes will be made")
        logger.info("=" * 80)
    
    logger.info(f"‚öôÔ∏è Configuration:")
    logger.info(f"   Partial albums: {'Enabled' if partial_albums else 'Disabled'}")
    logger.info(f"   Minimum files threshold: {min_files_threshold}")
    logger.info("=" * 80)
    
    # Get auth data from environment
    auth_data = os.environ.get('GP_AUTH_DATA')
    if not auth_data:
        logger.error("‚ùå GP_AUTH_DATA environment variable not set!")
        logger.error("   Set it with: export GP_AUTH_DATA='your_auth_data'")
        sys.exit(1)
    
    logger.info(f"‚úÖ Auth data found ({len(auth_data)} characters)")
    
    # Find manifest file
    manifest_path = 'ftp_structure_manifest.json'
    
    # Check if manifest exists
    if not os.path.exists(manifest_path):
        logger.error(f"‚ùå Manifest file not found: {manifest_path}")
        logger.error("   Please download it from GitHub Actions artifacts:")
        logger.error("   1. Go to Actions -> Generate FTP Structure Tree")
        logger.error("   2. Download the 'ftp-structure-manifest' artifact")
        logger.error("   3. Extract ftp_structure_manifest.json to this directory")
        sys.exit(1)
    
    # Initialize organizer
    organizer = PhotoOrganizer(auth_data, manifest_path)
    
    # Load manifest
    if not organizer.load_manifest():
        logger.error("‚ùå Failed to load manifest")
        sys.exit(1)
    
    # Initialize Google Photos client
    if not organizer.initialize_client():
        logger.error("‚ùå Failed to initialize Google Photos client")
        sys.exit(1)
    
    # Organize files
    logger.info("=" * 80)
    logger.info("üöÄ STARTING ORGANIZATION")
    logger.info("=" * 80)
    
    try:
        successful, failed, skipped = organizer.organize_files(
            dry_run=dry_run,
            partial_albums=partial_albums,
            min_files_threshold=min_files_threshold
        )
        
        logger.info("=" * 80)
        logger.info("üìä ORGANIZATION COMPLETE")
        logger.info("=" * 80)
        logger.info(f"‚úÖ Successful: {successful}")
        logger.info(f"‚ùå Failed: {failed}")
        logger.info(f"‚è≠Ô∏è Skipped: {skipped}")
        logger.info("=" * 80)
        
        if failed > 0:
            logger.warning("‚ö†Ô∏è Some files failed to organize - check logs for details")
            sys.exit(1)
        
        if dry_run:
            logger.info("üîç DRY RUN COMPLETE - No changes were made")
            logger.info("   Run without --dry-run to actually organize files")
        else:
            logger.info("‚úÖ Files have been organized into albums!")
            logger.info("   Check Google Photos to see your organized library")
            if skipped > 0:
                logger.info("")
                logger.info("‚ö†Ô∏è Note: Some files were skipped - see MISSING FILES REPORT above")
                logger.info("   These files can be added later by re-running this script")
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == '__main__':
    main()
