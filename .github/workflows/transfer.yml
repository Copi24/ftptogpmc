name: FTP to Google Photos Transfer

on:
  workflow_dispatch:
  schedule:
    # Run every 6.5 hours to continue processing (workflow timeout is 6h)
    # This ensures continuous processing across multiple runs
    # Pattern: Run at 00:00, 06:30, 13:00, 19:30 (every 6.5 hours)
    - cron: '0 0,6,12,18 * * *'      # Every 6 hours at :00
    - cron: '30 6,12,18 * * *'       # Every 6.5 hours offset (at :30)
  push:
    branches:
      - main

env:
  PYTHONUNBUFFERED: 1

jobs:
  transfer:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max
    
    steps:
      - name: Maximize build disk space
        uses: easimon/maximize-build-space@master
        with:
          root-reserve-mb: 1536        # Reduce to 1.5GB for system (frees ~500MB)
          swap-size-mb: 512            # Reduce swap to 512MB (frees ~500MB)
          remove-dotnet: 'true'        # Remove .NET (~10GB)
          remove-android: 'true'       # Remove Android SDK (~8GB)
          remove-haskell: 'true'       # Remove Haskell (~5GB)
          remove-codeql: 'true'        # Remove CodeQL (~2GB)
          remove-docker-images: 'true' # Remove Docker images (~5GB)
          temp-reserve-mb: 50          # Reduce temp reserve to 50MB (frees ~50MB)
          build-mount-path: '/workspace'
      
      - name: Additional aggressive cleanup
        run: |
          # Remove large unused packages and caches
          sudo apt-get clean
          sudo rm -rf /var/lib/apt/lists/*
          
          # Remove pip cache
          rm -rf ~/.cache/pip
          
          # Remove Python bytecode caches
          find /usr -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
          
          # Remove old logs
          sudo journalctl --vacuum-time=1d
          
          # Remove snap packages if any (can free several GB)
          if command -v snap &> /dev/null; then
            sudo snap list | grep -v "^Name" | awk '{print $1}' | xargs -r sudo snap remove 2>/dev/null || true
            sudo rm -rf /var/lib/snapd
          fi
          
          # Remove large unused tools
          sudo rm -rf /usr/local/lib/android 2>/dev/null || true
          sudo rm -rf /opt/hostedtoolcache/CodeQL 2>/dev/null || true
          
          # Show freed space
          df -h /
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          path: 'repo'
      
      - name: Download previous state (if exists)
        continue-on-error: true
        id: download-state
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd repo
          echo "üîç Looking for previous upload state..."
          
          STATE_FOUND=false
          
          # Priority 1: Try to download artifact from latest workflow run
          # Latest run should have all previous progress (since successful runs include all previous files)
          CURRENT_RUN_ID=${{ github.run_id }}
          LATEST_RUN=$(gh run list --workflow=.github/workflows/transfer.yml --status=completed --limit=5 --json databaseId --jq "map(select(.databaseId != $CURRENT_RUN_ID)) | .[0].databaseId")
          
          if [ -n "$LATEST_RUN" ] && [ "$LATEST_RUN" != "null" ]; then
            echo "üì• Found workflow run: $LATEST_RUN"
            echo "üîç Attempting to download upload-state artifact..."
            
            # Create temp directory for artifact extraction
            ARTIFACT_TEMP=$(mktemp -d)
            
            # Try to download the upload-state artifact from that run
            if gh run download $LATEST_RUN --name upload-state --dir "$ARTIFACT_TEMP" 2>/dev/null; then
              echo "‚úÖ Successfully downloaded upload-state artifact"
              
              # Artifacts are ZIP files - need to extract them explicitly
              ZIP_FILE=$(find "$ARTIFACT_TEMP" -name "*.zip" -type f -print -quit 2>/dev/null)
              
              if [ -n "$ZIP_FILE" ]; then
                echo "üì¶ Found artifact ZIP, extracting..."
                if unzip -q "$ZIP_FILE" -d "$ARTIFACT_TEMP" 2>/dev/null; then
                  echo "‚úÖ ZIP extracted successfully"
                  
                  # Look for upload_state.json in extracted contents
                  EXTRACTED_FILE=$(find "$ARTIFACT_TEMP" -name "upload_state.json" -type f -print -quit 2>/dev/null)
                  
                  if [ -n "$EXTRACTED_FILE" ]; then
                    # Copy extracted JSON to repo directory
                    cp "$EXTRACTED_FILE" ./upload_state.json
                    echo "üìÑ Artifact state file extracted: $(wc -c < upload_state.json) bytes"
                    # Count completed files for info
                    FILE_COUNT=$(jq -r '.completed | length' ./upload_state.json 2>/dev/null || echo "unknown")
                    echo "üìÑ Completed files: $FILE_COUNT"
                    STATE_FOUND=true
                  else
                    echo "‚ö†Ô∏è upload_state.json not found in extracted ZIP"
                    echo "üìÇ Listing extracted files:"
                    find "$ARTIFACT_TEMP" -type f | head -20 || true
                  fi
                else
                  echo "‚ö†Ô∏è Failed to extract ZIP file (unzip may not be installed)"
                  # Try installing unzip if not available
                  sudo apt-get update && sudo apt-get install -y unzip || true
                  if unzip -q "$ZIP_FILE" -d "$ARTIFACT_TEMP" 2>/dev/null; then
                    EXTRACTED_FILE=$(find "$ARTIFACT_TEMP" -name "upload_state.json" -type f -print -quit 2>/dev/null)
                    if [ -n "$EXTRACTED_FILE" ]; then
                      cp "$EXTRACTED_FILE" ./upload_state.json
                      echo "üìÑ Artifact state file extracted (after installing unzip): $(wc -c < upload_state.json) bytes"
                      FILE_COUNT=$(jq -r '.completed | length' ./upload_state.json 2>/dev/null || echo "unknown")
                      echo "üìÑ Completed files: $FILE_COUNT"
                      STATE_FOUND=true
                    fi
                  fi
                fi
              else
                # Try to find JSON file directly (in case artifact isn't zipped)
                DIRECT_JSON=$(find "$ARTIFACT_TEMP" -name "upload_state.json" -type f -print -quit 2>/dev/null)
                if [ -n "$DIRECT_JSON" ]; then
                  cp "$DIRECT_JSON" ./upload_state.json
                  echo "üìÑ Found artifact state file directly: $(wc -c < upload_state.json) bytes"
                  FILE_COUNT=$(jq -r '.completed | length' ./upload_state.json 2>/dev/null || echo "unknown")
                  echo "üìÑ Completed files: $FILE_COUNT"
                  STATE_FOUND=true
                else
                  echo "‚ö†Ô∏è No ZIP or JSON file found in artifact"
                  echo "üìÇ Listing artifact directory:"
                  ls -la "$ARTIFACT_TEMP" || true
                fi
              fi
              
              # Cleanup temp directory
              rm -rf "$ARTIFACT_TEMP"
            else
              echo "‚ö†Ô∏è No upload-state artifact found in previous run"
            fi
          else
            echo "‚ö†Ô∏è No previous workflow runs found"
          fi
          
          # Priority 2: Fallback to state file in repo (if committed)
          if [ "$STATE_FOUND" = "false" ] && [ -f "upload_state.json" ]; then
            echo "‚úÖ Found state file in repository (fallback)"
            echo "üìÑ Repo state file size: $(wc -c < upload_state.json) bytes"
            STATE_FOUND=true
          fi
          
          if [ "$STATE_FOUND" = "false" ]; then
            echo "‚ÑπÔ∏è No previous state found - will create new state on first run"
          fi
      
      - name: Check state file
        run: |
          if [ -f "repo/upload_state.json" ]; then
            echo "‚úÖ State file found"
            echo "State file size: $(wc -c < repo/upload_state.json) bytes"
            echo "Completed files: $(jq '.completed | length' repo/upload_state.json 2>/dev/null || echo 0)"
          else
            echo "‚ö†Ô∏è State file NOT found - will create new state"
          fi
        working-directory: .
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash
          rclone version
      
      - name: Setup GitHub CLI (for state uploads)
        run: |
          type -p curl >/dev/null || (sudo apt-get update && sudo apt-get install curl -y)
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \
          && sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \
          && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null \
          && sudo apt-get update \
          && sudo apt-get install gh -y
        continue-on-error: true
      
      - name: Configure rclone
        run: |
          mkdir -p ~/.config/rclone
          echo "${{ secrets.RCLONE_CONFIG }}" > ~/.config/rclone/rclone.conf
          chmod 600 ~/.config/rclone/rclone.conf
          echo "Rclone config created"
      
      - name: Test rclone connection
        run: |
          rclone lsd Challenger: || echo "Warning: Could not list directories"
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg p7zip-full unzip
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install https://github.com/xob0t/google_photos_mobile_client/archive/refs/heads/main.zip --force-reinstall
          pip install requests
      
      - name: Run transfer script (with periodic state upload)
        env:
          GP_AUTH_DATA: ${{ secrets.GP_AUTH_DATA }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        working-directory: repo
        run: |
          # Run script in background and periodically upload state
          python3 ftp_to_gphotos.py &
          SCRIPT_PID=$!
          
          # Upload state every 30 minutes while script runs
          while kill -0 $SCRIPT_PID 2>/dev/null; do
            sleep 1800  # 30 minutes
            if [ -f "upload_state.json" ]; then
              echo "üì§ Uploading state file (periodic backup)..."
              gh run upload ${{ github.run_id }} upload_state.json --name upload-state --pattern upload_state.json || true
            fi
          done
          
          # Wait for script to finish
          wait $SCRIPT_PID
          SCRIPT_EXIT=$?
          
          # Final state upload
          if [ -f "upload_state.json" ]; then
            echo "üì§ Final state file upload..."
            gh run upload ${{ github.run_id }} upload_state.json --name upload-state --pattern upload_state.json || true
          fi
          
          exit $SCRIPT_EXIT
        continue-on-error: true
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: transfer-logs
          path: |
            repo/ftp_to_gphotos.log
          retention-days: 7
      
      - name: Upload state for next run (final)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: upload-state
          path: repo/upload_state.json
          retention-days: 90
          overwrite: true
      
      - name: Check disk space
        if: always()
        run: |
          df -h
          du -sh ~/* 2>/dev/null || true

